{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Challenge \n",
    "\n",
    "<img src=\"https://imageio.forbes.com/specials-images/imageserve/5ecd179f798e4c00060d2c7c/0x0.jpg?format=jpg&height=600&width=1200&fit=bounds\" width=\"500\" height=\"300\">\n",
    "\n",
    "In the bustling city of Financia, the Central Lending Institution (CLI) is the largest provider of loans to individuals and businesses. With a mission to support economic growth and financial stability, CLI processes thousands of loan applications every month. However, the traditional manual review process is time-consuming and prone to human error, leading to delays and inconsistencies in loan approvals.\n",
    "To address these challenges, CLI has decided to leverage the power of machine learning to streamline their loan approval process. They have compiled a comprehensive dataset containing historical loan application records, including various factors such as credit scores, income levels, employment status, loan terms(measured in years), loan amounts, asset values, and the final loan status (approved or denied).\n",
    "\n",
    "\n",
    "**Your task is to develop a predictive model that can accurately determine the likelihood of loan approval based on the provided features. By doing so, you will help CLI make faster, more accurate, and fairer lending decisions, ultimately contributing to the financial well-being of the community.**\n",
    "\n",
    "It is recommended that you follow the typical machine learning workflow, though you are not required to strictly follow each steps: \n",
    "1. Data Collection: Gather the data you need for your model. (Already done for you)\n",
    "\n",
    "2. Data Preprocessing: Clean and prepare the data for analysis. (Already done for you)\n",
    "\n",
    "3. Exploratory Data Analysis (EDA): Understand the data and its patterns. (Partially done for you)\n",
    "\n",
    "4. Feature Engineering: Create new features or modify existing ones to improve model performance. (Partially done for you)\n",
    "\n",
    "5. Model Selection: Choose the appropriate machine learning algorithm.\n",
    "\n",
    "6. Model Training: Train the model using the training dataset.\n",
    "\n",
    "7. Model Evaluation: Evaluate the model's performance using a validation dataset.\n",
    "\n",
    "8. Model Optimization: Optimize the model's parameters to improve performance.\n",
    "\n",
    "9. Model Testing: Test the final model on a separate test dataset.\n",
    "\n",
    "**Please include ALL your work and thought process in this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may include any package you deem fit. We sugggest looking into Scikit-learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "loan_data = pd.read_csv(\"../../data/loan_approval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Uncomment to see desired output. Add more analysis if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loan_id', 'no_of_dependents', 'education', 'self_employed',\n",
      "       'income_annum', 'loan_amount', 'loan_term', 'cibil_score',\n",
      "       'residential_assets_value', 'commercial_assets_value',\n",
      "       'luxury_assets_value', 'bank_asset_value', 'loan_status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------ Display basic information ------\n",
    "print(loan_data.columns)\n",
    "print(loan_data.describe())\n",
    "\n",
    "# ------ Check for missing values ------\n",
    "print(loan_data.isnull().sum())\n",
    "\n",
    "# ------ Visualize the distribution of loan status ------\n",
    "loan_status_counts = loan_data['loan_status'].value_counts()\n",
    "plt.bar(loan_status_counts.index, loan_status_counts.values)\n",
    "plt.title('Distribution of Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# ------ Visualize the distribution of numerical features ------ \n",
    "loan_data.hist(bins=30, figsize=(20, 15))\n",
    "\n",
    "# ------ Correlation matrix ------\n",
    "corr_matrix = loan_data.corr()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(corr_matrix, cmap='coolwarm')\n",
    "fig.colorbar(cax)\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "\n",
    "# ----- MORE (Encouraged but not required) ------\n",
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "You may want to convert categorical variables to numerical. For example, education takes on the value Graduate and Not Graduate. But we want it to be 0 or 1 for machine learning algorithms to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['education'] = loan_data['education'].map({'Graduate': 1, 'Not Graduate': 0})\n",
    "# Hint: Other categorical variables are self_employed and loan_status\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = loan_data.copy()\n",
    "\n",
    "# Mapping education\n",
    "df[\"education\"] = (\n",
    "    df[\"education\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .map({\"graduate\": 1, \"not graduate\": 0})\n",
    ")\n",
    "\n",
    "# Mapping self_employed\n",
    "def yes_no_to_int(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"yes\", \"y\", \"true\", \"1\"}:\n",
    "        return 1\n",
    "    if s in {\"no\", \"n\", \"false\", \"0\"}:\n",
    "        return 0\n",
    "    return np.nan\n",
    "\n",
    "df[\"self_employed\"] = df[\"self_employed\"].apply(yes_no_to_int)\n",
    "\n",
    "# Map target loan_status -> y (Approved = 1, Denied = 0)\n",
    "def status_to_int(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"approved\", \"approve\", \"1\", \"yes\", \"y\", \"true\"}:\n",
    "        return 1\n",
    "    if s in {\"denied\", \"deny\", \"0\", \"no\", \"n\", \"false\"}:\n",
    "        return 0\n",
    "    return np.nan\n",
    "\n",
    "df[\"loan_status\"] = df[\"loan_status\"].apply(status_to_int)\n",
    "\n",
    "# Define features\n",
    "target_col = \"loan_status\"\n",
    "drop_cols = [\"loan_id\"] if \"loan_id\" in df.columns else []\n",
    "\n",
    "X = df.drop(columns=drop_cols + [target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(\"\\nFeature dtypes summary:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nX shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Class balance (y):\")\n",
    "print(y.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "You are free to use any classification machine learning models you like: Logistic Regression, Decision Trees/Random Forests, Support Vector Machines, KNN ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "# Preprocessing:\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Baseline models:\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "rf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": log_reg,\n",
    "    \"RandomForest\": rf\n",
    "}\n",
    "\n",
    "print(\"Baselines ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "def evaluate(model, X_tr, y_tr, X_va, y_va, name = \"model\"):\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    #Predictions\n",
    "    y_pred = model.predict(X_va)\n",
    "\n",
    "    # ROC-AUC\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_va)[:,1]\n",
    "        auc = roc_auc_score(y_va, y_proba)\n",
    "    else:\n",
    "        y_proba = None\n",
    "        auc = np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_va, y_pred),\n",
    "        \"precision\": precision_score(y_va, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_va, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_va, y_pred, zero_division=0),\n",
    "        \"roc_auc\": auc\n",
    "    }\n",
    "\n",
    "    print()\n",
    "    print(\"=== {} (Validation) ===\".format(name))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_va, y_pred))\n",
    "    print()\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_va, y_pred, digits=4))\n",
    "    print(\"Metrics:\", metrics)\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, mdl in models.items():\n",
    "    metrics, trained = evaluate(mdl, X_train, y_train, X_val, y_val, name=name)\n",
    "    results.append(metrics)\n",
    "    trained_models[name] = trained\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=[\"f1\", \"roc_auc\", \"accuracy\"], ascending=False)\n",
    "print(\"\\nSummary (sorted):\")\n",
    "print(results_df)\n",
    "\n",
    "best_name = results_df.iloc[0][\"model\"]\n",
    "best_baseline = trained_models[best_name]\n",
    "print(\"\\nBest baseline:\", best_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if best_name == \"LogisticRegression\":\n",
    "    param_grid = {\n",
    "        \"model__C\": [0.01, 0.1, 10],\n",
    "        \"model__penalty\": [\"12\"],\n",
    "        \"model__solver\": [\"lbfgs\"]\n",
    "    }\n",
    "    base_for_search = log_reg\n",
    "else:\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [300, 500],\n",
    "        \"model__max_depth\": [None, 6, 12],\n",
    "        \"model__min_samples_split\": [2, 8, 16],\n",
    "        \"model__min_samples_leaf\": [1, 3, 6]\n",
    "    }\n",
    "    base_for_search = rf\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_for_search,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(\"\\nBest params:\", grid.best_params_)\n",
    "print(\"Best CV f1:\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "else:\n",
    "    test_auc = np.nan\n",
    "\n",
    "print(\"\\n=== FINAL TEST RESULTS ===\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, y_test_pred, zero_division=0))\n",
    "print(\"ROC-AUC:\", test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
